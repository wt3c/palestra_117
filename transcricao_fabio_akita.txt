Olá pessoal, Fábio Akita. Esse é um episódio que eu tava tentando evitar fazer. Primeiro porque eu acho que vai ser um dos temas onde parte do vídeo vai acabar ficando obsoleto meio rápido, já que as tecnologias de IA estão acelerando e mudando bastante ainda. E segundo porque eu mesmo nunca trabalhei com IA. A maioria das coisas que eu explico nesse canal eu já me envolvi em projetos reais de alguma forma, por isso não é só teoria, é experiência. Mas IA eu brinquei, mas nunca trabalhei, nem fiz pesquisa acadêmica, nem nada disso, por isso nunca me senti adequado pra explicar. Mas considerando que quase a totalidade de vídeos feitos sobre o assunto hoje são de pessoas com menos conhecimento ainda, fazendo afirmações cada vez mais absurdas, até eu certamente consigo fazer muito melhor. Pra variar, os oportunistas já saíram lançando cursos e afins, e no fim do vídeo eu vou explicar porque todos são uma droga e você deve evitar. E eu também vou explicar de novo porque ela não vai substituir programadores. Hoje eu vou explicar o que de fato é um chat GPT e onde estamos quando se fala em IA. Então vamos lá. Esses dias eu resolvi brincar de IA e o objetivo era ter o meu próprio chat GPT rodando offline, totalmente local, na minha máquina, sem conectar com nenhuma API de terceiros como o da OpenAI. E isso ficou fácil porque agora existem diversos esforços da comunidade de código aberto, em particular projetos de OpenLLM ou Large Language Models, que é a categoria de IA onde se encontra um GPT. Esses esforços ganharam força quando a Meta decidiu abrir seu modelo LAMA em fevereiro de 2023. As Big Tech tem diversos modelos prontos, cada um pra algum uso específico, desde respostas gerais ou mais focadas em código e assuntos específicos. Por exemplo, a OpenAI tem os modelos GPT-3, GPT-4, Codex, a Microsoft tem o Zero, Megatron em conjunto com a NVIDIA. O Google tem modelos como o BERT, Palma, Lambda, Minerva e outros. E a Meta de novo tem o OPT, Galactica, MetalM, LAMA. Alguns desses modelos são citados em papers, mas são fechados, como o novo Bard do Google ou o próprio GPT-4 da OpenAI. Mas o lançamento do LAMA ao público foi um evento importante e a Meta parece que liberou vários outros, como o OPT e Galactica. E a primeira parte do vídeo de hoje vai ser entender o que diabos são esses modelos. Isso foi em fevereiro. Esse vídeo tá saindo em junho, só 4 meses. E de lá pra cá a comunidade usou o LAMA pra criar derivados mais otimizados e menores pra rodar em computadores menos parrudos. Surgiram diversas variantes como Alpaca, Vicuna, GPT-4ALL, Koala, Dolly e dezenas de outros, principalmente dezenas. O site Hugging Face, que é um repositório de modelos e ferramentas pra inteligência artificial, tem listado uns 200 modelos diferentes. E o que dá pra fazer com esses modelos abertos? Deixa eu mostrar. Eu queria um chat GPT pessoal, offline, que ninguém sabe o que eu tô conversando com ele, sem filtro, sem controle, sem nada. Só eu e a ferramenta. Existem vários projetos abertos que implementam interface web similar ao chat GPT, onde eu posso digitar perguntas, ver as informações da IA e ficar conversando com ela sem precisar estar conectado na internet. Olha só esse de aniversário. E aí 1 E aí Se assistiram o meu episódio sobre games em máquina virtual, sabem que eu tenho uma máquina que alguns consideram parruda, um AMD 5950X de 16 cores, 32 threads, rodando uns 4 GHz por core, mais 4 GB de RAM DDR4, uma GPU Nvidia RTX 3090 com 24 GB de memória GDDR5, fora meu NAS Synology de 60 TB conectado em rede de 10 GB. E meus testes foram feitos numa máquina virtual QEMU rodando o Ubuntu normal com PassFru de PCI pra ter acesso direto no NVIDIA. Todos os detalhes sobre essa QEMU eu mostrei no vídeo de games em máquina virtual. Sem nenhum motivo em particular, eu escolhi o projeto aberto Text Generation Web UI, que é uma aplicação web escrita em Python. Ela simula a interface web de chat do chat GPT. Por baixo carrega bibliotecas como bits and bytes, PyTorch e outros que eu vou explicar depois. Daí podemos baixar modelos e fazer ele carregar um deles. E no caso eu tô rodando Vicuna 30B quantizado. De novo eu já vou explicar o que isso significa. Dizem que tem uma qualidade parecida com GPT-4, mas pra mim pareceu mais um GPT-3. As respostas que eu consigo no GPT-4 ainda são mais completas do que nesse Vicuna. Mas mesmo assim é impressionante. Tão vendo? Eu tô conversando de boas e não tá conectado com nenhum serviço online de ninguém, nem da Microsoft, nem da OpenAI, nem do Google. Tudo rodando offline, local, dentro da minha máquina virtual. E eu vou repetir porque quando postei sobre isso no Instagram, muita gente ficou confusa. Sim, dá pra rodar um programa similar ao serviço de chat GPT da OpenAI num computador normal. E não se trata de uma demonstração Hello World, realmente funciona. Não precisa de um monte de servidores parrudos pra conseguir isso. Ué, mas eu achava que precisava ter o tamanho de uma Microsoft ou Google pra fazer isso. O que mudou? A exigência de hardware depende da complexidade do modelo. Como eu falei, eu tô usando Vicuna de 30 bilhões de parâmetros. Mas existem modelos menores, como o próprio Vicuna de 7 bilhões de parâmetros ou a ADA da Microsoft de só 350 milhões. E se o modelo for pequeno o suficiente é possível até rodar num bom smartphone Android ou um Raspberry Pi com upgrade de RAM. E o tamanho do modelo é um dos fatores que podem afetar a qualidade das respostas. Portanto, quanto menor o modelo, mais simples seriam as respostas. Eu simplifiquei esse tamanho. Guardem essa informação que eu já já vou explicar mais. Deixa eu recapitular um pouco. O que diabos é isso de modelo? Honestamente, eu mesmo não sei dizer em todos os detalhes pra isso precisaria ter estudo e treinamento em inteligência artificial, em particular redes neurais. Muita gente aprendeu isso em optativas ou iniciação científica na faculdade de ciências da computação. Eu não vou tentar dar explicação acadêmica, mas sim dar uma intuição pra maioria de vocês conseguirem ter uma imagem na cabeça. E acadêmicos, sejam compreensivos. E se quiserem complementar, sintam-se à vontade nos comentários abaixo. Pense em redes neurais como uma simulação do aprendizado que acontece no nosso cérebro. Sabemos que temos neurônios. Aprendizado e memória acontece quando temos sinapses, conexões desses neurônios, ou comunicação ou melhor, ativação de um neurônio, que é como se ele escolhesse um caminho dentre muitos. E um neurônio pode ter milhares de conexões, dizem que umas 7 mil. E de novo, explicação grosseira. Mas é mais ou menos isso que temos em redes neurais. Assista aos vídeos do canal 3Blue1Brown sobre redes neurais pra entender em mais detalhes. No caso específico de texto, poderíamos pensar que a forma de fazer computadores aprenderem a interpretar e gerar texto seria cadastrando regras gramaticais, ortográficas, vocabulário, dicionários, e assim ele conseguiria construir frases gramaticalmente corretas. Mais ou menos como você pensa que é o jeito certo de aprender uma língua nova, como inglês ou francês num curso qualquer. Esse parece ser o jeito intuitivo, certo? Mas se você assistiu meu vídeo de como eu aprendi inglês e a minha live com o Matt sobre aprender japonês, já sabemos que não é assim. Não se pode decorar dúzias de regras. Nem no aprendizado em geral, nem em inteligência artificial. Não existem regras, como um monte de ifs. Pense por 2 segundos. Quando foi a última vez que você escreveu um texto 100% gramaticalmente e formalmente correto? Concorda que um texto assim seria super estranho? Sem gíria, sem maneirismo, sem coloquialismo, com palavras consideradas rebuscadas, exatamente o que associamos a gente. Eu já falei isso nesses outros vídeos, mas vou repetir. Como você aprendeu português? Foi lendo um livro do tamanho da bíblia, lotado de regras gramaticais, quando tinha 1 ou 2 anos de idade? Como um bebê que nasceu nos Estados Unidos aprendeu inglês? Como que um bebê que nasceu na China aprendeu chinês? Nenhum deles usou nenhum livro, nenhum curso, nenhuma regra. Simplesmente passaram um tempão ouvindo os pais e pessoas ao redor e começaram a repetir o que ouviam. Bem errado no começo. Todo mundo vai dando feedback. Quando o bebê tinha uma intenção, sei lá, dizer que tá com fome, ele tentava juntar palavras que já tinha ouvido antes e que parecia descrever o que queria. Se os adultos ao redor dessem comida, é o feedback que o que ele falou fazia sentido. E assim ele vai associando as combinações de palavras com comportamentos, vai refinando seu aprendizado, fazendo novas sinapses, novas conexões e pouco a pouco melhorando a comunicação e se fazendo entender a grosso modo é como seria o que chamamos de treinamento supervisionado no mundo de inteligência artificial. Nós não programamos regras gramaticais, nem cadastramos palavras num banco de dados manualmente e ficamos fazendo ifs, ifs pra montar frases. Em vez disso começamos com um corpo de dados gigante, por exemplo, todos os artigos da Wikipedia, todos os códigos abertos disponíveis no GitHub, todos os papers acadêmicos disponíveis publicamente, todos os livros digitalizados no Google Books, bastante texto, dezenas ou centenas de gigabytes de texto puro. Pra entender isso deixa eu fazer uma tangente e explicar um conceito relacionado que não é em si só inteligência artificial, mas faz parte da matéria. Na faculdade de ciência da computação se aprende sobre processos estocásticos que estuda aleatoriedade, probabilidades ou melhor, de uma evolução de um sistema ou fenômeno ao longo do tempo de forma probabilística. Ele descreve o comportamento de um sistema ou quantidades que mudam aleatoriamente ao longo do tempo. Em particular, eu quero falar de cadeias de Markov. Eu não tô falando que o chat GPT é uma cadeia de Markov, é só pra ilustrar um ponto. Cadeias de Markov é uma das formas de representar e analisar sequências de eventos ou estados onde a probabilidade de transicionar de um estado pra outro depende somente do estado atual. É um sistema sem memória ou backtracking, sem considerar a sequência de todos os estados anteriores, só o último. Em resumo, é um conjunto de estados e probabilidades de transição. Se parecer o grego, vamos ver um exemplo prático. Digamos que em vez de ter gigabytes de textos como descrevi antes, nosso corpo de treinamento sejam só três frases em português. Eu gosto de comer maçãs. E daí? Ela gosta de jogar tênis. E finalmente? Ele prefere ler livros. Podemos construir um modelo baseado nessas frases onde os estados são palavras ou tokens e as transições entre estados representam a probabilidade de mover de um token pra outro. Vamos construir esse modelo simplificado. As transições são probabilidades. Esses são exemplos, mas digamos que a transição do token ou estado eu pro estado gosto é probabilidade 1, ou seja, 100%. Mas a transição do estado de, pra comer é 0,5 ou 50%, porque poderia ser pra jogar, que é 0,5 também. Pra ficar claro, na primeira frase temos de comer, mas na segunda frase temos de jogar. Por isso a probabilidade do token de, temos duas possibilidades 50% de chance pra cada. Como temos poucas frases de treino, as transições são quase 100% de uma palavra pra outra, porque esse modelo só conhece três frases. Num treinamento de verdade, com gigabytes de transição, teríamos trilhões de possibilidades diferentes e probabilidades pequenas e fracionadas, como 0.0001234 blá blá. Finalmente, digamos que começamos a digitar um texto e queremos que esse modelo continue completando a frase pra gente. Podemos usar o modelo da cadeia de Markov pra prever o que seria a palavra mais provável, baseado nas probabilidades de transição que vimos na lista anterior. Por exemplo, eu começo digitando EU e a probabilidade da próxima palavra ser GOSTO é 100%, então é isso que ele dá de previsão. Se digitar ELE, a probabilidade segundo a lista é 100% pra PREFERE. Por causa de smartphones, todo mundo já viu isso em ação de verdade. É a funcionalidade de auto correção que tem em todo o teclado. Olhe nesse exemplo. Começo digitando uma palavra e o teclado sozinho vai sugerindo a próxima palavra. Podemos só aceitar a sugestão. E ele vai sozinho completando a frase. E claro, se ficar fazendo só assim, a frase vai ficando meio sem sentido nenhum. Mas ele consegue gerar uma frase que mais ou menos parece um humano que escreveu, não acha? Eu vou repetir, isso é uma explicação simplificada. Tem várias outras técnicas em cima de cadeias de Markov, mesmo pra um tecladinho simples de iOS ou Android. Mas em linhas gerais, pense que em vez de três frases, o modelo desses teclados foi pré-treinado com milhares de frases. E o modelo é essa lista de combinações de palavras e as probabilidades da próxima palavra dada uma palavra anterior. Essas probabilidades é o que chamamos de pesos. E mais importante, em nenhum momento usamos quaisquer regras hardcoded de gramática ou ortografia ou ifs ou templates. Ele vai completando a frase puramente usando esses pesos, aprendidos no treinamento. E nada mais. Agora vamos voltar pro chat GPT ou pra minha versão local do Text Generation com Vicuna. Vocês nunca acharam estranho que as respostas sempre demoram e ele vai escrevendo uma palavra de cada vez? Alguns poderiam achar que é só uma animação arbitrária pra fazer parecer que o chat GPT é uma pessoa digitando. Mas deixa eu rodar uma versão fora da interface web, na linha de comando mesmo. Prestem atenção. Pra ficar mais claro, vou colocar do lado o monitoramento da minha GPU, a ferramenta da Nvidia, chamado Nvidia SME, que faz o monitoramento dos recursos sendo usados na GPU. Notem que durante a composição da resposta a GPU tá em uso constante, processando alguma coisa sem parar. Eu não sei porque usa só 50% do processamento disponível, mas de qualquer forma a resposta não é instantânea. Não é uma animação feita só pra fazer graça. É que ele demora isso mesmo palavra a palavra. Cada palavra nova que vai aparecendo tá gastando processamento da GPU. Se levou 5 segundos pra dar resposta, foi 5 segundos que a minha GPU ficou processando sem parar. Conseguem ver as similaridades entre o autocorretor do seu tecladinho de celular e o processo de resposta do chat GPT? Internamente ele tá fazendo algo similar a procurar probabilidades na cadeia de Markov. Mas claro, o modelo de GPT, Lama, Vicuna, Bard e outros é mais complicado que um mero modelo de Markov. Vamos entender. Quando as primeiras notícias anunciando o chat GPT saíram, gerou muita confusão que persiste até hoje. Por exemplo, quando o chat GPT 4 foi anunciado eles mencionam Uau, o chat GPT 3.5 tinha 175 bilhões de parâmetros, mas o chat GPT 4 tem incríveis 100 a 170 trilhões de parâmetros. A única coisa que os jornalistas e vocês entenderam foi Uau, bilhões pra trilhões de... whatever. Trilhões. É absurdamente maior que bilhões. Então o GPT novo é milhões de vezes melhor e o GPT 5 vai ser mais milhões de vezes melhor. E é assim que todo mundo mente e se auto engana com números, sem saber o que significa. Esse meu Vicuna rodando localmente na minha máquina tem meros 30 bilhões de parâmetros. Puts, quer dizer que ele deve ser pelo menos 5 vezes pior que o chat GPT antigo, né? Nem chega aos pés do GPT 4. Só que se olhar alguns artigos que descrevem o Vicuna menor, o de 13 bilhões de parâmetros, muitos declaram que rodando diversos testes, os mesmos que a própria OpenAI usa pra avaliar o GPT deles, dizem que o Vicuna 13B chega a 90% do nível de qualidade do GPT 4 ou Google Bard. E o Vicuna, sendo derivado do lama do Facebook, nesses mesmos testes ultrapassa o lama original. Como que pode isso? Antes de mais nada, o que diabos são esses tais parâmetros? No contexto de machine learning, parâmetros se referem aos pesos que o modelo aprende durante o processo de treinamento. De forma simplificada e grosseira, lembra a lista de probabilidades de transição de estados da cadeia de Markov que mostrei no exemplo? Aquilo poderíamos chamar de parâmetros. No nosso treinamento com só 3 frases gerou um modelo de 16 parâmetros. Pense numa lista como aquela só que com bilhões de linhas, 13 bilhões ou 30 bilhões no caso do Vicuna, ou 170 trilhões no caso do chat GPT 4. E lembra como no exemplo, dado uma palavra, podemos ir na lista de probabilidades e ver qual poderia ser a próxima palavra? A grosso modo é como GPT ou Vicuna fazem. Só que em vez de considerar só a palavra anterior e procurar a próxima, ele vê quais foram as palavras anteriores e leva todas em consideração pra tentar prever a próxima palavra. Lembra como minha GPU fica processando sem parar enquanto monta a resposta? É isso que ele tá fazendo, pesquisando no modelo pela próxima palavra, mas considerando parte ou todo o texto anterior, incluindo as palavras que ele mesmo sugeriu. Aí vai ficando cada vez mais pesado e por isso demora. E antes que o pessoal acadêmico me crucifique, é melhor eu me corrigir aqui. Eu repeti várias vezes, a grosso modo, a grosso modo. Porque o modelo do GPT não são probabilidades de pares de palavras, daquele jeito bonitinho como no exemplo. Intuitivamente poderíamos pensar que quando se quebra um texto longo teríamos conjuntos de palavras. Mas vamos recordar a metáfora do bebê aprendendo? Todo mundo já deve ter percebido que um bebê não ouve um adulto falar uma palavra e sai repetindo bonitinho, igualzinho, certo? Parte da diversão é justamente ver ele falando errado no começo e ir ajustando. O bebê tenta reproduzir o que ele acha que ouviu. Daí você dá o feedback negativo e ele vai tentando de outras formas até uma hora acertar. No mundo de Machine Learning e Deep Learning podemos usar isso como metáfora. No treinamento ele não isola palavras, isola patterns, funcionalidades ou features do material de treinamento. Fora de IA, no mundo de Full Text Search ou em processamento de linguagem natural, temos ferramentas como Elastic Search ou Apache Solar que eu expliquei no episódio do Twitter. Eles não quebram os textos indexando palavras, mas sim Grams. Quem já leu a documentação deve ter visto o termo Ngram, que são sequências de N itens ou tokens. Pode ser uma palavra inteira, mas pode ser só parte de uma palavra. Uma sequência de só uma letra seria um unigram, duas letras um bigram, três letras trigram e assim por diante. Quando indexamos texto é mais útil indexar Ngrams do que palavras inteiras. É o que nos permite fazer coisas como achar palavras parecidas ou que soam parecido, que tem mesmo sufixo ou mesmo prefixo como conjugação de verbos. Mesma coisa com o teclado de autocorreção, ele indexa Ngrams. Por isso você digita a pesquisa tudo errado no Google, mas ele diz no resultado, você quis dizer X. O mundo de indexação de textos é todo baseado no conceito de Ngrams. Vale a pena estudar isso depois. Quer dizer que o tal modelo do GPT são pesos em cima de Ngrams? Infelizmente também não é assim fácil. Eu mencionei Ngrams só pra explicar como podemos dividir palavras de outras formas não intuitivas. Agora, o problema com sinapses no nosso cérebro ou redes neurais é que os pesos não são aplicados em cima de ideias discretas, como palavras ou letras ou imagens inteiras. E de novo, pra entender tecnicamente como redes neurais funcionam, procure material de universidades como do MIT, Stanford ou lugares assim. E de novo, acadêmicos, peguem leve comigo. No fim do dia, um modelo é como se fosse um banco de dados contendo probabilidades ou pesos de um elemento pra outro elemento, parecido com o exemplo da autocorreção. Mas o principal é que não são necessariamente palavras, não são também só Ngrams, pode ser qualquer tipo de pattern que foi identificado no aprendizado. Pode ser uma letra pra uma palavra, pode ser um bigram pra um trigram, pode ser muitas coisas. Por exemplo, poderia ser EF pra YW, probabilidade 0.01234. O que isso significa? Isoladamente não significa absolutamente nada. Ela só vai fazer sentido dentro de uma rede de pesos. A probabilidade final é uma composição de múltiplos passos pelos nós dessa rede. O modelo não é uma lista. Provavelmente é mais como um espaço vetorial multidimensional, tipo matrizes de matrizes. Eu expliquei espaços vetoriais no episódio do Twitter também. Similaridade de cosseno, álgebra linear, lembram? E de novo, pra visualizar, não pense num modelo como sendo uma lista, como um array simples. Pense como um array de arrays de arrays. Multidimensional. Se multidimensional não é intuitivo pra vocês, pense num elemento simples primeiro, tipo uma variável de tipo inteiro. Isso é o que chamamos de valor escalar, com zero dimensões. Representa um único valor. Agora, um array de escalares, como um array de inteiros, isso é um vetor, uma lista de uma única dimensão. E em seguida, em vez de escalares, se eu fizer um array de arrays, onde cada elemento do array é outro array, e esse array for unidimensional, agora temos uma matriz, que é um retângulo ou uma grade de valores escalares. Finalmente, e se esse array interno também tiver arrays como elementos? Agora temos um array de arrays de arrays tridimensional. E podemos ir adiante. Esse último array também pode ter arrays de elementos, que tem arrays de elementos, que tem arrays de elementos, aí temos matrizes multidimensionais ou, mais corretamente, tensores de auto-ordenamento ou tensores n-dimensionais. Aliás, tudo isso que eu expliquei são tensors, sabe? Do tal TensorFlow do Google. Um escalar é um tensor de zero dimensões. Um vetor é um tensor de uma dimensão. Uma matriz é um tensor de duas dimensões. E acima disso é um high order tensor ou n-dimensional tensor. Então, voltando pro tal modelo do GPT-Ovicuna, não pense como no exemplo simples do autocorretor, que foi somente um vetor, um array unidimensional de pesos. Pense que esses pesos estão estruturados em tensores n-dimensionais. Todos esses conceitos que eu vim explicando não são a ponta do iceberg, são a raspa da ponta do iceberg. Pode parecer que eu quero dizer que um chat GPT não é mais que uma cadeia de Markov só que maior. E não é isso. É só uma metáfora pra explicação. Deixa eu tentar explicar qual foi a tal revolução que permitiu o salto de um mero autocorretor pra um chat GPT. Mas entenda, redes neurais e deep learning existem faz décadas. A comunidade de ciências da computação vem fazendo descobertas e refinando as tecnologias faz muito tempo. Não foi do nada que isso apareceu. Seguindo ainda o exemplo do autocorretor, deve ser fácil de perceber um dos problemas. Ele só usa a palavra anterior pra tentar descobrir qual a próxima palavra. Por isso fica muito rapidamente a frase sem sentido. Dá a impressão que foi um humano que escreveu, mas um humano bem burro. É diferente de um punhado de palavras completamente aleatórias, mas as frases que gera são bem inúteis. Quanto maior tentar fazer a frase, pior vai ficar. Claro, o certo é que a próxima palavra leve em consideração não só a palavra anterior, mas todas as palavras anteriores pra manter a coerência. É isso que poderíamos chamar de Backtracking ou Recurrency. Isso tem que ser levado em conta durante o treinamento. Não basta quebrar o texto em palavras e só fazer o peso da palavra seguinte. Tem que ser o peso da palavra seguinte dado as palavras anteriores. E é aí que nascem coisas como RNN ou Recurrent Neural Networks. RNNs foram desenhados pra lidar com sequências de tamanhos variados como sentenças, dados de séries de tempo, sinais de discurso. Eles conseguem processar inputs como um texto um passo de cada vez e ao mesmo tempo mantendo um estado interno escondido que mantém informações dos passos anteriores. Ou seja, ele mantém memória durante o aprendizado. É como a gente aprende também. Uma coisa é aprender. Por exemplo, a palavra foda, dependendo do contexto, pode significar coisas diferentes. Pode ser que signifique, putz, que foda esse Macbook novo. Ou seja, positivo. Ou pode ser, putz, que trampo foda de difícil. Ou seja, negativo. E várias outras variações. Precisamos de contexto. E contexto precisa de memória pra gerar pesos diferentes pra contextos diferentes. Consegue imaginar o trampo de processar pensando dessa forma? RNNs usam técnicas como BPTT ou Back Propagation Through Time. Literalmente, propagação reversa através do tempo pra computar gradientes e atualizar os parâmetros do modelo. Então não é um processamento que você pega um texto, lê só uma vez do começo ao fim e já gera um modelo escrito linearmente do começo ao fim. Tem que ficar voltando pra trás no modelo pra ajustar. E já que é pra dificultar em todo o paper de IA por aí, vamos esbarrar nesse termo. Gradiente. Deixa eu resumir. Gradiente se refere à derivada da loss function, função de perda ou de custo. E putz, Akita, eu nunca vou usar cálculo na vida. É perda de tempo. Bom, eis um pequeno exemplo. Pra que serve derivada? Ela serve pra medir a taxa de mudança de uma função. E em resumo a derivada nos diz como o resultado de uma função muda à medida que fazemos pequenas modificações nos valores de entrada. Lembra de física cinemática no colegial? Fórmula pra saber o espaço ao longo do tempo? Fórmula de velocidade ao longo do tempo? A de velocidade é derivada da fórmula de espaço. Porque velocidade é a taxa de mudança da fórmula de espaço ao longo do tempo. A aceleração é a taxa de mudança da fórmula de velocidade. Em particular, gradientes em cálculo de múltiplas variáveis é um vetor que aponta na direção da ascendente mais íngreme da função. Isso é super importante em otimização de algoritmos, como gradient descent ou descida de gradiente, que iterativamente atualiza os parâmetros pra encontrar a função de custo mínimo. Dado que gradientes nos ajudam a entender a taxa de mudança de resultados de uma função, podemos usar pra encontrar os pontos de máxima e mínima pra otimização. E esse conceito é importante em otimização de machine learning. Em machine learning tem um troço que chamam de loss function, também conhecido como função de custo, que é uma função matemática que quantifica a discrepância entre resultados previstos de um modelo com os valores de verdade. E lembrando, uma das formas de treinar é dar um monte de dados de treinamento, daí pedir pro modelo devolver respostas a várias perguntas e ver se as respostas estão corretas, justamente pra calibrar o aprendizado. Lembra do bebê aprendendo a falar e olhando pra nossa cara pra ver se a gente entendeu? Tipo isso. O objetivo do tal treinamento é ver quão bem um modelo performa determinadas tarefas. A escolha de qual função de custo usar depende do problema que queremos resolver, como regressão, classificação, geração de sequências. Por exemplo, em tarefas de regressão uma função de custo popular é o MSE ou Min Squared Error, erro quadrado mediano, ou o MAE que é Min Absolute Error, erro absoluto mediano. Em tarefas de classificação tem custo de entropia cruzado e assim por diante. O importante é entender que treinamento não é um troço aleatório, tem funções de métrica e controle pra calibrar e otimizar. Durante o treinamento os parâmetros do modelo são ajustados pra minimizar essa função de custo usando algoritmos de otimização como o Gradient Descent. Agora vocês entendem uma das formas que cálculo influencia a qualidade do treinamento de um modelo de rede neural. E pra hoje pense em rede neural como uma caixa preta, que nem uma função que você programa na sua linguagem de programação. Ela tem variáveis de entrada e algum retorno. A entrada seria as tais toneladas de textos pro treinamento. Daí no meio, nessa caixa preta, esses dados são processados de alguma forma e o retorno vai ser o tal modelo multidimensional. Essa etapa no meio é o processamento dos textos e materiais de treinamento que passamos. Processos como tokenização, que é quebrar o texto em listas de sequências de palavras. São uma série de transformações pra massagear esses dados em diversas camadas escondidas. Essas transformações envolvem cálculo, por exemplo, soma ponderada. Cada neurônio de uma camada escondida recebe inputs de uma camada anterior e a cada input é assinalado um peso. O neurônio computa a soma ponderada dessas entradas onde os pesos determinam a significância da contribuição de cada input pra saída do neurônio. Daí a soma ponderada pode ser passada pra uma função de ativação que introduz não-linearidade e determina a saída da função. Não-linearidade em sistemas complexos é um assunto gigantesco, nem vou tentar explicar. Mas pra ter uma intuição, pense assim. Você tá acostumado a pensar em sistemas lineares. Por exemplo, se um litro de gasolina dá pra andar 15 quilômetros, então 10 litros de gasolina vai dar 150 quilômetros. Mas não-linearidade é que nem tentar prever o tempo. Só porque temos 80% de umidade no ar e no passado vimos que isso indicava uma chuva de, sei lá, 20 milímetros, não quer dizer que se eu medir hoje 80% vai dar os mesmos 20 milímetros. Pode ser 40, pode ser zero. Tem uma rede de outras variáveis, algumas mensuráveis, algumas desconhecidas, uma influenciando a outra. Variáveis minúsculas podem amplificar resultados completamente inesperados. É o famoso caso da borboleta de Lorenz, aquela história que uma borboleta bate as asas no Brasil e tem um tsunami no Japão. Não foi a borboleta que causou o tsunami. Não é linear, não tem causalidade direta. Mas quer dizer que essa minúscula contribuição, somada a milhares de outras, pode ter causado o tsunami. Depois parem pra ler sobre teoria do caos, é fascinante. E o mundo real é cheio de efeitos não-lineares. E levamos isso em conta em redes neurais. O nosso cérebro tem aprendizado não-linear. E é o que tentamos simular com redes neurais. Agora, Deep Learning, como o próprio nome diz, é aprendizado profundo. E profundidade se refere a várias camadas de aprendizado. Aquela função caixa preta que eu mencionei, imagine várias delas em série. Uma chamando a outra, várias camadas de profundidade. Lógico, explicação simplificada, mas só pra dar uma noção. Enfim, a parte importante é que processar texto em Deep Learning usando técnicas como RNN e BPTT seria absurdamente caro pra manter toda a memória e fazer todo esse backtracking na força bruta. É aí que entra o famoso paper do Google, Attention is all you need. Literalmente, atenção é tudo que você precisa, publicado por Vasone e equipe em 2017. É o paper que introduz a arquitetura de Transformers, os famosos transformadores que permitiram essa geração de LLMs como GPT. Em pouquíssimas palavras, ele introduz o mecanismo de Self-Attention, ou Auto-atenção, também conhecido como Atenção Escalada de Produto Escalar. Eu expliquei produto escalar no contexto de espaços vetoriais no episódio do Twitter, não vou explicar de novo. Mas esse mecanismo permite o modelo pesar a importância de diferentes posições na sequência de entrada, possibilitando capturar efetivamente as dependências longas. E eu sei, é difícil de entender isso e também pra eu explicar. Mas lembra como RNN precisa manter um estado em memória pra lembrar o contexto? Self-Attention é uma otimização disso. Em vez de ser Recurrent Neural Network, ele passa a poder usar Feed Forward Neural Network. Da forma como eu entendo, em vez de um processo onde você dá um passo pra trás antes de poder dar um passo pra frente, agora é só passos pra frente, Feed Forward. O que possibilita isso é o tal mecanismo de Auto-atenção, elimina a necessidade de recorrência ou convolução, e ainda permite paralelizar o processamento. Antes, como o passo seguinte dependia do passo anterior, tinha que ser feito em série, em sequência linear. E parte da dificuldade de conseguir rodar coisas em paralelo é eliminar as dependências que amarram o passo seguinte com o passo anterior. Isso vale não só pra IA, mas qualquer coisa. Alternando o processo Feed Forward, evitando convolução, podemos paralelizar o processamento. O que levaria chutando um ano pra treinar, poderia ser feito em um mês. E o importante é entender que essa arquitetura de transformers é uma otimização massiva. É mais ou menos o tipo de impacto que você vê num desenvolvimento web comum quando coloca um índice numa tabela gigante. Ou quando coloca um cache na frente do banco de dados e ganha 5, 10 vezes a performance. Independente de como funcionar no detalhe, o importante é entender que foi um salto grande. Toda hora eu fico falando que os acadêmicos vão me matar vendo essas minhas explicações grosseiras, mas eu mesmo fico doído de ficar toda hora falando a grosso modo, simplificando, em metáfora. Porque cada parágrafo que eu falei até agora são dúzias de papers e formalidades matemáticas. Eu tô tentando trazer um pouco desse vocabulário pra vocês entenderem que não é um chute do nada, mas também reduzir em poucas palavras que ajudem a dar uma intuição. Pra maioria de nós os detalhezinhos não importam tanto. Tem mais valor ter a noção desse processo em linhas gerais pra entender que não é mágica. E principalmente qual é o limite dessa mágica. Mas com tudo isso que eu falei, vamos tentar entender o que é o chat GPT então. Como é um projeto proprietário, fechado e secreto da OpenAI, temos que acreditar nas informações que eles disponibilizaram. Então sempre leiam isso com vários quilos de sal. E isso dito, parece que o treinamento foi baseado num corpo de aproximadamente 570 gigabytes de texto. Quais textos exatamente não sabemos, mas eles mencionam Wikipedia, artigos de pesquisa e papers, websites e outras formas de conteúdo escrito na web com limite até 2021. E isso é arbitrário. Pessoalmente achei pouco texto, eu teria chutado mais, mas 570 gigabytes só de texto puro é bastante coisa na real. Por exemplo, a Wikipedia inteira dá um total de 21 gigabytes. E isso eu acho que é contando com o HTML que monta as páginas. Se filtrar e limpar só os textos puros vai ser bem menos. Mas digamos que seja 21 gigabytes. Precisaria de mais de 30 Wikipedias inteiras pra completar os 570 gigabytes de dados de treino. É um volume respeitável. Esse tanto de texto dizem que deu um total de 300 bilhões de palavras. Entre aspas. Mas eu acho que o jornalista entendeu errado. Um dicionário de inglês como o Merriam Webster Online não tem meio milhão de palavras. Eu acho que são 300 bilhões de tokens, que incluem palavras, mas também Engrams, como eu falei antes, e seja lá quais outros patterns o Deep Learning identificou nesse material. Daí passa por semanas fazendo todo o processo que eu falei de transformação. Esse processo que levaria meses, agora parece que dura mais ou menos um mês rodando em não sei quantos servidores usando hardware como os agora famosos Nvidia Grace Hopper, os GH100. Lembra que eu falei que internamente não estamos lidando com valores escalares e sim com tensores multidimensionais? CPUs como uma Intel, a AMD que roda no seu PC, mesmo os M1 ou M2 da Apple, são chips com instruções feitas pra cálculos em cima de valores escalares. Uma função de soma pega dois valores inteiros de 64 bits e cosme um resultado inteiro de 64 bits. Eu vou ver como isso funciona nos episódios de emuladores como o do Super Mario com processadores de 8 bits, então depois deem uma olhada. CPUs modernas incluem instruções pra lidar com vetores. Instruções SIMD, Single Instruction Multiple Data. Literalmente uma instrução pra múltiplos dados. Começou com as instruções MMX dos primeiros Pentium nos anos 90. Hoje temos conjuntos de instruções como SSE4 ou AVX512. Pra ter a intuição, em vez de uma função que recebe o inteiro, pense numa outra função que recebe dois arrays. Soma os dois arrays e cospe um array resultante. Tudo numa única instrução. GPUs, diferente de CPUs, não tem capacidade de rodar qualquer programa genérico. Lembram do meu episódio de Turing Complete? A grosso modo, uma máquina de Turing é basicamente qualquer programa. Em particular, pra ser Turing Complete, pra ser um computador moderno, ele precisa ser capaz de rodar um programa que consegue simular ser um computador. Como exemplo mais óbvio, pense numa máquina virtual. Ele nem precisa conseguir rodar na prática, mas tem que ter a capacidade teórica. Num CPU ARM M2 da Apple é possível simular uma CPU Intel usando Rosetta. E esse programa de Intel roda achando que tá num PC de verdade. Isso é possível porque uma CPU ARM M2 é Turing Complete. Já uma GPU não tem essa capacidade. Diferente de CPUs que são genéricos e podem simular qualquer coisa, mesmo que lento, uma GPU é um hardware especializado pra executar um conjunto pequeno de tarefas bem definidas. Uma GPU não consegue rodar um sistema operacional, genérico, nem simular ser outra GPU, tipo uma AMD Radeon tentar simular ser um NVIDIA RTX. Não funciona assim. Quando existe camada de abstração, quem cuida disso é a CPU. A GPU é boa numa única coisa, fazer cálculos de vetores e matrizes. Uma CPU Intel costuma tecelar 8, 16, 32 núcleos com duas threads cada rodando a 4 ou 5 GHz hoje em dia. Mesmo chips de servidores como o Intel Xeon ou AMD Epic não tem muito mais que isso de cores. Já uma GPU é diferente. Uma novíssima RTX 4090 tem nada menos que 16 mil núcleos pra Shade de CUDA, 128 núcleos pra Ray Tracing, nada menos que 512 núcleos exclusivos só pra tensors. Uma GPU diferente de uma CPU tem milhares de núcleos que rodam em clocks baixos, como 1 GHz pra funções altamente especializadas. O que eu falei que é o resultado do treinamento de Deep Learning? Um modelo de tensors N-dimensionais. O que foi feito pra calcular tensors multidimensionais? GPUs? Processar áudio, processar vídeo, processar polígonos ou voxels tridimensionais é tudo processamento de matrizes multidimensionais. Uma tela de computador ou do seu smartphone como que é representado? Num monitor Full HD é um array de 1080 colunas onde cada elemento é um array de 1920 elementos pra cada linha. Se eu quiser escurecer essa imagem inteira, pode ser uma subtração em cada valor desses arrays, fazendo isso adicionando ou subtraindo uma matriz por outra que chamamos de um filtro ou kernel. Numa CPU você programaria como um loop nas colunas e outro loop nas linhas pra calcular a nova cor, pixel a pixel. Seria 1920x1080 operações ou mais de 2 milhões de operações. Numa GPU eu passo a matriz inteira e ele calcula tudo numa única operação. E de novo, eu não vou conseguir entrar em detalhes, mas essa é a diferença fundamental. De curiosidade, um dos maiores problemas dessa arquitetura de CPU, controlando GPU, é o compartilhamento de memória. Em PCs modernos a CPU tem um conjunto de RAM e a GPU tem um conjunto de VRAM separados. A CPU prepara os dados e tem que mandar pra GPU processar. Daí uma vez calculada a CPU precisa puxar o resultado de volta pra própria RAM. No frigir dos ovos essa comunicação é um gargalo. Não é raro vermos jogos, por exemplo, que perde frames, mesmo a GPU não estando em 100%, mas se olhar a CPU é ela que está em 100%, então vira um gargalo e a GPU fica um tempo parado esperando. Por isso desde a nona geração de consoles de videogames como PS5 e Xbox Series X se falou tanto em loads instantâneos e tecnologias de melhorar esse gargalo como Microsoft Direct Storage. Também é por isso que a estratégia da Apple com os chips M1 e M2 é ser um SOC ou System on a Chip, um único chip que embute CPU, GPU e RAM tudo junto pra minimizar ao máximo esse gargalo. Junta tudo no mesmo lugar, garante o uso mais eficiente de memória e caminho mais curto de comunicação ajudando a evitar gargalos. Na velocidade que estamos hoje, a distância da sua CPU Intel pros pentes de RAM é gigante se comparado a soldar tudo junto no mesmo chip como a Apple faz. E é um saco porque não dá pra aumentar RAM depois. Mas a razão não é porque eles são uma corporação querendo arrancar mais dinheiro de vocês, mas sim porque tirar esse gargalo faz muita diferença. A mesma coisa acontece na solução pra data centers da NVIDIA, a tal arquitetura Gracie Hopper que eu falei. Começa com o super chip Gracie Gracie, tanto Intel quanto ARM, que eles mesmos desenvolveram num único pacote com NVLink que é um barramento de altíssima velocidade entre eles, e a alternativa Gracie Hopper que é outro super chip que junta uma CPU Gracie com uma GPU H100 Hopper. São soluções que juntam todos esses chips junto com meio terabyte de memória RAM LPDDR5 de 32 canais. Estamos falando de 96 núcleos de 3 nanômetros. É um monstro. É esse o produto que tem feito as ações da NVIDIA disparar, porque eles encaixam perfeitamente pra acelerar processamento de transformers. O meu PC não é parrudo. Esse da NVIDIA sim é a verdadeira definição de parrudo, o atual estado da arte em 2023. É com servidores desse tipo, não sei quantos, que se pega meio terabyte de dados, quebramos em 300 bilhões de tokens e no final a OpenAI consegue gerar um modelo de GPT-4, com os tais 170 trilhões de parâmetros. Queria entender melhor agora essa frase? Daí a mídia e os jornalistas ficam assustados e noticiam como o GPT-4 se iguala ao cérebro humano, que tem 100 trilhões em sinapses, lembram? Sinapses mais ou menos são os pesos ou parâmetros entre neurônios. É agora que entendemos mais ou menos o que são esses parâmetros, vamos discutir a premissa errada. Parâmetros não são equivalentes a sinapses do cérebro humano. Quando se joga números arbitrários assim no título de uma matéria, todo mundo fica empolgado. Não me deu erro. Pra começar, o nosso cérebro em média tem uns 100 bilhões de neurônios. E de novo, eu não sou um neurologista, então já assuma que minha explicação vai ser simplificada e de alto nível. E em pesquisa de IEA estamos tentando igualar neurônios biológicos com neurônios digitais numa rede neural. E falamos em bits como num computador. Mas pra começar, neurônios não são exatamente binários assim. Um único neurônio é capaz de lidar com múltiplos sinais e conexões. Em termos de sinapse, pode ter até umas 7 mil. Isso eu acho que seria o máximo. Mas em média o cérebro é capaz de ter até uns 600 trilhões de sinapses. Eu não sei porque, dependendo de onde se pesquisa, falam em 100 trilhões, outros falam em 600 trilhões. E de novo, precisa pesquisar um pouco mais a literatura de neurologia pra entender o que isso significa. Isso não é um valor absoluto. Tem vários fatores. Doenças como Alzheimer, por exemplo, afeta justamente a capacidade de fazer e manter sinapses. Em crianças, quando o cérebro ainda é muito mais elástico e não foi limitado pelo crescimento, o potencial é de um quadrilhão de sinapses. É um número absurdo. Mas, mais importante, os 170 trilhões de parâmetros do GPT-4 não se equipara à quantidade de sinapses que nosso cérebro é capaz ainda. Portanto é falso que o GPT-4 já se igualou ao cérebro humano em quantidade de sinapses. Outra premissa é errada. Um parâmetro de modelo de GPT não é equivalente a uma sinapse do cérebro humano, nem de longe. Usamos vocabulário neurológico pra simplificar a descrição em termos de inteligência artificial como redes neurais. É uma metáfora. Em nenhum momento nenhum cientista da computação vai te dizer que um neurônio de rede neural é idêntico ou sequer próximo de um neurônio biológico. É só uma abstração. E no caso de IA, parâmetros de modelos de transformers diferentes têm pesos diferentes. Parâmetros não são iguais entre modelos. Vamos voltar ao meu Vicuna rodando localmente na minha máquina. Lembram da afirmação do povo que fez teste de otimizou esse modelo? Um modelo Vicuna de 13 bilhões de parâmetros, em muitos casos, chega até 90% da qualidade de respostas de um chat GPT-4 de 170 trilhões de parâmetros. Como isso é possível? E por isso eu mencionei não linearidade e teoria do caos. Um único parâmetro isolado, se tentar ler e interpretar, não tem como inferir nada. Só funciona se tiver combinado com vários outros parâmetros numa rede. O resultado final depende da interação de múltiplos parâmetros e por isso se gasta processamento da GPU pra gerar uma resposta. Parâmetros são pesos, probabilidades. E tem várias formas de otimizar isso. Por exemplo, probabilidades pra ter o máximo de precisão podem ser valores escalares de tipo float de 32 bits. É isso que se gera no modelo depois do treinamento. As pesquisas mostram que podemos truncar esses valores pra float de 16 e a qualidade das respostas não cai drasticamente. É uma forma de otimização. Simplificando, é parecido com o conceito de música em MP3, que dados das frequências que o ouvido humano não é capaz de detectar são cortados fora. Tecnicamente isso tira a qualidade do áudio, mas na prática a maioria dos humanos não sente. Em termos de armazenamento economizamos, sei lá, 10 vezes o espaço fazendo isso. Eu expliquei isso no episódio de 25 TB pra 5 GB, onde pegamos uma imagem bruta em bitmap e reduzimos pra um JPEG. A qualidade cai, mas o olho humano sem treinamento não nota diferença tão grande assim. Otimizações e compressão são formas de simplificar os dados, diminuir a qualidade de forma que nossos sentidos sem treinamento não sintam tanta diferença significativa. Fazemos isso com modelos de IA também. E os valores de Float32 pra Float16 é uma forma de quantização. Existem várias formas de quantização que são otimizações dos modelos pra exigir menos processamento pra gerar respostas sem danificar demais a qualidade. Isso ajuda a conseguir fazer um clone de chat GPT como o Vicuna rodando uma máquina caseira como a minha. Parece que minha RTX 3090 já é quase topo de linha, mas não. Por isso eu expliquei sobre a Nvidia GH100, que é o tipo de hardware necessário pra rodar o chat GPT de verdade. Mas respondendo, como um Vicuna de 13 bilhões de parâmetros consegue competir com um GPT-4 de 170 trilhões? E é porque além de quantização, os algoritmos de autoatenção tem evoluído também. Em autoatenção, que acontece no processo de treinamento, cada token numa sequência precisa ser considerado com todos os outros tokens pra capturar dependências e relacionamentos. Tipicamente, autoatenção é computado dentro de uma janela de contexto, onde cada token é considerado com tokens vizinhos. Agora tem uma variante chamada autoatenção global, onde cada token é considerado independente de posição ou distância, que permite o modelo capturar dependências num contexto mais global. Na prática é assim, custa mais caro pra treinar, mas os parâmetros resultantes no modelo tem mais qualidade. Então com menos parâmetros conseguimos chegar em respostas de qualidade similar, entenderam? A qualidade de um parâmetro não é universal nem estático, ele tá mudando à medida que aperfeiçoamos os algoritmos de treinamento e estruturas de dados. Redes neurais, mesmo Deep Learning com autoatenção global, ainda são representações rudimentares e grosseiras do nosso cérebro. Pra um GPT ou Vicunda da Vida conseguir escrever um texto com estilo de Shakespeare, precisamos treinar com todos ou quase todos os textos do Shakespeare. Pra conseguir gerar uma música parecida com Mozart, temos que dar o máximo de composições de Mozart quanto possível. O processo de treinamento vai encontrar patterns ou padrões e criar pesos pra eles, registrando no modelo, mas isso ainda é bem ruim se comparado ao cérebro humano. Pense você, se estiver treinando em literatura ou música, mesmo não lendo nem perto de todas as obras de Shakespeare, mesmo não estudando nem de perto todas as composições do Mozart, rapidamente consegue começar a copiar o estilo deles. Veja você como programador, não precisou ler todos os códigos já feitos em React pra começar a escrever códigos, bastou uns dois tutoriais. O nosso cérebro consegue aprender muito melhor que um Transformer, com muito menos dados e produzir resultados similares ou melhores dentro de um mesmo determinado assunto. E A hoje em dia ainda depende muito de força bruta. Desde que o Lama foi lançado em fevereiro de 2023, estamos só em junho e já temos dezenas de modelos diferentes com vários níveis de quantidade de parâmetros, caindo desde 65 bilhões até só 7 bilhões, com vários tipos de otimização como GPTQ pra quantização ou Float 16. Isso permite rodar algo parecido com esse meu VQ na local no Android ou até no Raspberry Pi. É o que eu acho ideal, um Transformer rodando localmente, offline, sem compartilhar nenhum dado pessoal meu, nenhuma conversa com nenhuma corporação por aí. Quanto menos dados meus eu tiver que dar pra alguém, melhor. Eu acho que é realmente ser se alguém não me paga por isso, pelo contrário, pra usar OpenAI eu preciso pagar a assinatura e sabe-se lá o que fazem com as minhas conversas. Bacana, significa que se o jornaleiro fala que o GPT-4 é próximo já de um ser humano e os nerdolas da ciência da computação já compactaram e tornaram eficientes até esse ponto do Vicuna, então já era. Tá fácil fazer Skynet né? Afinal, GPT-4 já consegue escrever código de programação. Basta eu carregar o código do GPT-4 e mandar ele melhorar o código e gerar o GPT-5. Daí eu pego o novo GPT-5 e faço ele fazer uma versão melhor, GPT-6. E assim sucessivamente até eu ter o GigaChad GPT-T1000 Skynet que vai dominar o mundo, certo? Errado. No final do dia um GPT-5, 10, 20 continua não sendo mais que um autocorretor do seu teclado em versão maior. Tirem da cabeça a noção de que é uma inteligência, ela não é inteligente. Tentar definir inteligência é um buraco de coelho profundo demais, vai ter gente masturbando filosofia aqui até o fim dos tempos e ninguém vai chegar numa conclusão. Vamos só assumir que até hoje não temos uma definição exata de inteligência humana. Na verdade quando chamamos de inteligência artificial só quer dizer que os resultados para um ser humano podem parecer com algo inteligente, mas não que de fato é inteligente. Entendem a diferença? Não ter uma definição exata é ruim, porque não temos um plano exato de pra onde ir. Na neurologia de verdade, apesar dos avanços, não temos uma receita exata de como neurônios e sinapses funcionam 100%. Temos uma boa ideia, mas muita coisa ainda é especulativo. Mas podemos dar alguns chutes educados baseado em tudo que eu falei até agora. E aqui vai ser minha opinião pessoal. Se alguém tiver pesquisas que discordam, sinto-se à vontade pra linkar nos comentários abaixo. Opinião por opinião, cada um pode ter a sua. E a maioria parece estar pulando rápido demais pra religião do AGI. E eu vou ser advogado do diabo e dizer porque isso não tá nem perto de acontecer. Vocês entenderam até aqui? Quando vamos na interface web do chat GPT ou Vicuna ou qualquer outro derivado e escrevemos o tal do prompt a pergunta, damos uns segundos e ele nos traz uma resposta. Mas na realidade não é isso. O que vem na verdade é uma continuação do texto do prompt. Tem uma diferença importante aqui. Ele não tá tentando te responder. Acontece que com os parâmetros treinados no modelo, a probabilidade maior das próximas palavras é se parecer com uma resposta. Essencialmente o que tá acontecendo é similar ao autocorretor do teclado do seu celular. Dado o texto que acabou de digitar, quais palavras tem mais probabilidade de serem continuação? Não é essa sutileza. Por isso eu falo que não é uma inteligência. GPT ou Vicuna não são pessoas e nem entidades com cognição tentando se comunicar. É meramente um completador de textos. Ou mais tecnicamente, um transformer pré-treinado generativo. Um gerador de texto. Uma pergunta ou prompt é um texto que se digita na expectativa que o modelo consiga continuar completando. É por isso que o texto da resposta vai aparecendo aos poucos. Não é uma animação arbitrária. É igual você ficar clicando na próxima palavra sugerida pelo teclado. Um dos parâmetros que afeta essa continuação se chama temperatura. Nós não temos controle da temperatura do chat GPT pela interface web, mas no Bing da Microsoft tem esses controles de mais balanceado, mais preciso ou mais criativo. E no meu Vicuna eu tenho esse campo numérico. Novamente, do jeito que todo mundo escreve, parece que estamos configurando uma pessoa pra ser mais criativa. Mas isso é só parte do showzinho. Um transformer não é mais ou menos criativo. Ele é mais ou menos aleatório. Temperatura controla a aleatoriedade do complemento de texto sendo gerado, o que você chama de resposta. O modelo assinala probabilidades pra cada possível token, que são candidatos pra ser a próxima palavra na sequência. Alta temperatura, valores maiores do que 1.0, significa que o modelo assinala probabilidades similares a um conjunto maior de tokens. Se vários tokens têm probabilidades parecidas, as respostas podem variar mais quando se repete a mesma pergunta. Quanto maior a temperatura, mais você vai achar que ele tá sendo mais criativo, mas também aumenta a probabilidade dele começar a dar resposta sem sentido. Temperatura média entre 0.5 a 1 é a resposta balanceada do Bing. É a mesma coisa que alta temperatura, mas ele assinala probabilidades similares pra um conjunto menor de tokens, controlando um pouco mais o que se perceberia como criatividade. E baixa temperatura, que é abaixo de 0.1, faz o modelo assinalar probabilidades similares pra palavras que realmente tinham mais chance de ser o próximo token. Isso faz a resposta parecer mais focada, determinística, previsível. Respostas sem sentido. Muitos chamam de alucinação. Eu não gosto desse termo porque implica que se a alucinação é temporária nas respostas, então no resto do tempo ele tá sendo sóbrio ou racional. E não tá. Todas as partes das respostas foram geradas mediante probabilidades do modelo. Eu só ajusto a quantidade de aleatoriedade nos candidatos pra próxima palavra. Nada mais, nada menos. Ele nunca alucina assim como nunca é sóbrio. Ele não é uma entidade consciente pra ser nenhuma das duas coisas. É só um programa obedecendo probabilidades armazenadas num modelo. Entenda, no frigir dos ovos, criatividade no mundo de Transformers é só uma métrica de aleatoriedade, não tem nada a ver com criatividade humana. Assim como inteligência artificial não tem nada a ver com cognição de verdade. Transformers e todo tipo de machine learning ou deep learning são meros sacos de probabilidades. Eles não tem de fato cognição pra pensar. Hum, os dados dizem que a probabilidade desse evento é X, mas realmente faz sentido? Tem alguma coisa que eu não tô percebendo? Deixa eu parar pra pensar. O que seria o que um ser humano inteligente consegue fazer? Transformers não refletem, só cospem exatamente o que as probabilidades do treinamento do modelo dizem pra ele cuspir. E nada mais. Quando um Transformer consegue pegar uma equação e achar a resposta, ou pegar até uma planilha de balanço de uma empresa e dizer se tá indo bem ou mal, ela não tá sendo inteligente. Simplesmente existem probabilidades no modelo que levam a resposta porque materiais que ele usou de treinamento tinham já a resolução de equações parecidas. Da mesma forma que eu disse que não existe um monte de ifs de regras gramaticais pra conseguir escrever textos, também não tem um monte de ifs de regras matemáticas pra fazer contas. Não se volte à imagem daquela lista de pares de palavras e probabilidades do exemplo do teclado com alto corretor. Aquilo é tudo que ele tem. Um conjunto gigante de probabilidades. Toda resposta que te dá, por mais inteligente que pareça, não teve nenhum tipo de raciocínio, ou lógica, ou inferência, nem nada. Só probabilidades. Um texto gerado por um Transformer às vezes pode parecer simpático. Se você for uma pessoa carente, vai parecer que o Transformer responde se importando com você. Mas nada disso é intencional. É você projetando nele o que gostaria que ele fosse. Todo mundo faz isso com animais de estimação. Isso se chama antropomorfismo. Você pode atribuir emoções humanas pra animais ou objetos inanimados. Tem gente que jura que uma escultura sorri pra ele quando passa na frente. Obviamente não. Um exemplo simples disso são as nuvens do céu. Vira e mexe, você olha pro céu e vê claramente uma escultura nas nuvens. Dependendo da sua inclinação ideológica, poderia pensar Uau, Deus realmente é criativo. Olha que obra maravilhosa nos céus que ele tá me dando. Ou Uau, a mãe natureza é especial. Mãe Gaia continua a demonstrar sua genialidade criativa nos céus. Ou Uau, a droga que eu acabei de tomar é da hora. Só esse último pode tá certo. O fato é que a nuvem em si não tá em formato de nada. Se passar um avião por cima da mesma nuvem não vai ver nada, ou vai ver outra imagem. Da posição que você tá, na sua cabeça, suas sinapses pré-treinadas sugerem que você tá enxergando a silhueta de algum objeto ou animal que já viu antes. É um efeito colateral da nossa cognição. Poderíamos chamar de bug. Esse é o cérebro primitivo. O cérebro rápido, automático e meio burro. Ninguém moldou essa nuvem. Ela aleatoriamente acabou numa determinada posição que parece ter um determinado formato. Não houve intenção. Foi aleatório. Eu diria que a IA tá pouco se fudendo pra você, mas isso implicaria que ela tem consciência. Então ela nem tá se fudendo pra você. Ela não pensou em você. Mesma coisa com o Transformer. Ela não tem consciência. Ela simplesmente é. Uma mera ferramenta. Uma chave de fenda que sabe cuspir palavras segundo uma matriz de probabilidades. Nada mais, nada menos. Por acaso você tá olhando pra chave de fenda e achando que tá sorrindo pra você. Não tá. Isso diz mais sobre o seu estado emocional do que sobre a ferramenta. Significa que ela não tem insights, não tem momento eureca de descobrimento de coisas novas. Ela só é capaz de sugerir uma palavra depois das outras palavras que vieram antes, seguindo uma tabela de probabilidades que vieram do treinamento feito com esses gigabytes de texto. De vez em quando dá a impressão que ela criou algo que não existia, mas não é criação. É aleatoriedade, pura sorte, é a nuvem no céu. Não é um processo repetível e ela não sabe refletir algo como UAU, isso que eu inventei é daora, nunca tinha visto antes. Ela não tem emoções pra conseguir dizer UAU. Se você der o código de uma IA feita com RNNs, como eu falei antes, pro GPT-4 analisar, ela não vai magicamente conseguir chegar no paper de Transformers. Se esse paper não tava no material de treinamento ela não vai concluir sozinha que esse era o próximo passo. Portanto ela é incapaz de conseguir chegar num GPT-5. Novas descobertas precisam ser feitas por humanos, documentadas, alimentadas no treinamento de IA e só aí ela vai saber cuspir o texto desse paper. Ela é incapaz de usar o que aprendeu pra gerar descobertas novas de forma intencional. Existe uma historinha que é chamada do Teorema dos Macacos Infinitos. O teorema sugere que se tiver um macaco apertando teclas numa máquina de escrever por uma quantidade infinita de tempo, quase com certeza vai conseguir digitar qualquer tipo de texto, incluindo todas as obras do Shakespeare. Na prática estamos falando de uma quantidade tão absurda de tempo que seria impossível até de vocês terem noção. Estamos falando provavelmente de mais tempo que a idade atual do universo desde o Big Bang. Mas é um conceito que demonstra que sim, sem nenhuma inteligência, só com aleatoriedade, dado um tempo absurdo, uma hora tudo que já produzimos vai aparecer, sem nenhuma intenção, só via aleatoriedade. É uma história pra dar noção de conceitos como infinito e aleatoriedade. Essa história é creditada ao matemático francês Félix-Émile Borel de 1913, ou seja, desde o começo do século 20 já se tinha essa noção que muitos de vocês, mais de um século depois, ainda estão com dificuldades de entender. GPT e derivado são macacos modernos dessa história. Só que em vez de totalmente aleatório, damos um modelo de probabilidades pra facilitar o trabalho deles, só isso. Mas se chamar de macaco é meio ofensivo, pro coitado do macaco, que ainda é mais inteligente que qualquer IA. Falando em século passado, deixa eu aproveitar pra voltar lá atrás na história das inteligências artificiais. O estudo de algoritmos e técnicas que contribuem pra esse campo existe desde os anos 50 pelo menos. Mas nos anos 70 surgiu um programa que até hoje ainda deixa os desavisados meio surpresos. O nome do programa é ELISA. Existem várias versões, inclusive rodando online. Deem uma olhada. O relacionamento há przecar o período. A Medienquistica da Vega 말á. A seize o ponto que neste caso permite việc a Conhecer a Qu -"". Como podem ver, é um chatbote também que nem o chat GPT, e parece uma pessoa bem desconfiada, dando umas respostas meio grossas, o Ibis de anime chamariam ela de tsundere. Mas se hoje o povo fica empolgado com o chat GPT respondendo, imagina isso nos anos 70. Só pra dar contexto, isso é antes da revolução dos microcomputadores de 8 bits como Commodore ou Apple II. Isso é antes da Microsoft ou Apple existir. Nem internet existe ainda. Quando eu rodei uma versão de Elisa pra DOS lá no começo dos anos 90, lembro que eu fiquei fascinado e pensando como eu faria a minha própria versão. E aí vocês podem se perguntar, caraca, mas como era possível ter isso nessa época? Bom, quem faz ciências da computação ou programa há um tempo já deve ter entendido o funcionamento. O modelo é supersimples, é baseado inteiramente em pattern matching. Basicamente encontrar palavras chaves, ver se tem uma resposta pré-programada e montar essa resposta na mão. É um sistema de templates. Quando eu digo na mão, aqui sim. Ao contrário de modelos de transformers, é realmente um monte de ifs, ifs, ifs. Quem já montou agentes ou robozinhos, tanto de chat de suporte ou email marketing, já fez coisa similar pra respostas automáticas. É um bom exercício de faculdade pra iniciantes. No GitHub vai achar várias versões. Veja essa, uma versão feita em Java. E vamos ver o tal modelo, entre aspas. É uma lista de chaves e valores. E os valores, como podem ver, são frases pré-prontas. Por exemplo, se durante o chat você se desculpe e escreve algo como I'm sorry, ele quebra essa string, encontra a palavra sorry e nessa lista, olha só, tem três respostas pré-programadas. Ela pode responder, por favor, não se desculpe ou desculpas não são necessárias. E vai escolher aleatoriamente pra não parecer que tá se repetindo. Só isso já é suficiente pra passar no teste de Turing, que é um teste feito pra identificar quando uma inteligência artificial consegue enganar um ser humano numa breve conversa. Elisa é a avó espiritual do chat GPT. Mas o ponto em apresentar a Elisa é pra vocês verem que pra enganar seres humanos não precisa de muito. Por alguma razão nós seres humanos somos muito fáceis de enganar e somos propensos a acreditar em qualquer coisa, acho que somos animais com fé excessiva. É um ponto forte, mas é um enorme ponto fraco também. Eu pessoalmente sou cético, mas aí me chamam de do contra. Vai entender? Vou continuar não sendo trouxa, só isso. Um chat GPT, se eu quisesse simplificar bastante, não é muito diferente da Elisa em conceito. É um programa que usa um modelo, um dicionário de probabilidades que mostra frases de acordo com o que se digitou antes, roboticamente, automaticamente, pegando palavras cujos valores de probabilidade fazem mais sentido dado as palavras anteriores. E não existe nenhuma emoção envolvida, não existe simpatia, não existe compaixão, não existe amargura, nada. Não tem uma linha de código, nem de dados no modelo que representam qualquer emoção. É apenas uma calculadora que em vez de devolver números devolve conjuntos de palavras. Só isso. Mas e as IAs que geram? Imagens inéditas, video próprio DALE 2 da OpenAI, video MIDI Journey, video Stable Diffusion, video novos plugins de geração e ajuste de imagem proprietário da Adobe que começaram a ser distribuídos no Creative Cloud, classificação de imagens, geração de novas imagens, são campos diferentes dentro da inteligência artificial. E só pra não perder o gancho, deixa eu jogar na mesa mais alguns conceitos. Um campo que existe desde pelo menos os anos 80 é o estudo sobre CNN. E não, não é esse não, é uma CNN que é útil. Convolutional Neural Networks, que é um modelo que dizem que é bom pra processar dados que se parecem com grades, como uma grade de pixels, que é como representamos uma imagem. As tais camadas convolucionais, não sei se é assim que fala em português, aplicam vários filtros ou kernels aos dados de pixels pra criar um mapa de funcionalidades. Em resumo, esses filtros extraem características da imagem, o equivalente a tokenizar um texto e gerar palavras ou anagrams. Mas CNNs ficaram super famosos só depois de 2012, quando saiu a AlexNet de Alex Krzyzewski e Lia Sutskiver e Jeffrey Hinton. Devo ter arregaçado os nomes, mas beleza. Era um concurso que todo ano o vencedor ficava um pouco melhor do que o do ano anterior. Mas em 2012 o salto foi uma ordem de grandeza melhor. Não lembro os números, mas faz de conta que todo ano os melhores algoritmos conseguiram identificar 85% das imagens, no ano seguinte 86% e do nada deu um salto pra 99%. Isso gerou um enorme interesse na comunidade de pesquisa em cima de classificação de imagens. Em paralelo, em 2014 surgiu o conceito de GANs, Generative Adversarial Networks ou Redes Generativas Adversárias desenvolvido pelo Ian Goodfellow e seus colegas. Foi um avanço no conceito de geração de imagens. De forma simplificada é como se fossem duas IAs uma competindo com a outra. Um gerador cria imagens e um discriminador avalia. Por exemplo, digamos que quero gerar imagens de gatos. O gerador faz as imagens e o discriminador tenta identificar se é um gato mesmo. É um processo que acelera o processo de aprendizado por fornecer feedbacks muito mais rápidos do que um treinamento supervisionado por humanos. CNNs e GANs eu mencionei mais pra vocês saberem alguns nomes importantes, mas o DALE da OpenAI não usa GANs. Ele usa um derivado do modelo de transformers do GPT, só que aplicado a imagens. Assim como geração de respostas de texto, onde ele vai prevendo uma palavra após a outra, levando o contexto anterior em consideração, o DALE também usa uma arquitetura de transformers criando imagens um pedaço atrás do outro. Ou seja, também usa mecanismos de auto-atenção em vez de camadas recorrentes ou convolucionais tradicionais. Essa não é a única forma de se criar um gerador de imagens. Mais do que o DALE, que é fechado, eu gosto do Stabled Fusion que assim como o lama da meta também foi liberado publicamente como modelo aberto. Quem me acompanha no Instagram viu quando eu fiquei brincando de usar o Stabled Fusion com outras ferramentas abertas pra fazer remasterização, upscaling de video. Assim eu pegava um video super antigo com qualidade de DVD ou VHS e conseguia fazer ele redesenhar uma versão 4K. Se não viu isso, vejo o destaque usando o IA no meu Insta. E enfim, diferente do DALE que usa transformers, o Stabled Fusion, como o próprio nome diz, um modelo de difusão. Esse modelo gera imagens simulando um processo aleatório rodando em reverso. Ou seja, o processo de geração começa com uma simples distribuição, como um Gaussian Noise. Literalmente um gerador de barulho aleatório mesmo. E vai gradualmente refinando esse barulho passo a passo até chegar numa imagem que parece com os dados que o modelo foi treinado. Aqui como partir de um punhado de argila tudo bagunçado e passo a passo ir esculpindo e refinando até chegar numa escultura. Esse processo de refinamento é dirigido por uma rede neural que aprende a predizer o próximo passo da difusão. E por isso ele se chama Stabled Fusion ou difusão estável. Um site que parece que tem ganhado relevância na comunidade de pesquisa e desenvolvimento de ferramentas e modelos de IA é o Hugging Face. Ele serve como um repositório, é de lá que podemos baixar os modelos de lama, alpaca, vicuna, em todos os diferentes tamanhos e formatos. Existem modelos específicos pra texto como vicuna, específico pra imagens como o do Stabled Fusion, conversor de texto pra áudio como o Spit T5 do Facebook e muitos outros. O ponto é que não existe uma única inteligência unificada que faz tudo. Existem modelos isolados e independentes feitos pra tarefas específicas. Um GPT não sabe converter texto em áudio, isso é outro modelo. Portanto, quando vê produtos web que parecem uma única inteligência, não é. É um integrador que divide o que você pediu entre diversas inteligências diferentes. O que me leva a outro tema que eu queria tocar de leve, mesmo porque eu só comecei a estudar isso recentemente. Eu expliquei como as coisas funcionam superficialmente por baixo dos panos, mas agora precisamos falar do que fica por cima dos panos. A gente vai ter um novo tipo de interface, mas também a interface de APIs e tudo mais. Surgiu um framework que tem ganhado cada vez mais relevância na comunidade que tá investindo pra construir ferramentas integradas com os diversos serviços de IA que eu mencionei, como GPT, BARD, Stabled Fusion e outros. Esse framework se chama LangChain. A grosso modo, pense em um framework como Django, Laravel, Rails ou Spring, mas feito pra construir aplicativos que usam essas IAs. Eu gostei particularmente da documentação. Pra um projeto de código aberto novo tem material suficiente de estudo, incluindo pesquisa de papers acadêmicos sendo publicados agora. Em particular, eu queria tocar no ponto de prompts. A moda dos parasitas sanguessuga agora é criar cursos online de prompts. Já viram por aí? Vire um engenheiro de prompt. É a coisa mais idiota que eu já vi. Seria o equivalente a você se chamar de engenheiro de pesquisa no Google. Já deixo a dica pra vocês, não faça nenhum deles. Sem nem olhar eu posso garantir que quase todos são pega-trouxa. Lembra aquele ditado? Todo dia um malandro e um otário acorda e vão pra rua. E quando se encontram rola negócio. Não seja o otário. Enfim, o LangChain é um framework extenso. Não pense em algo simples como um Express de Node.js. Isso tá mais pra um Spring de Java. Tem diversos conceitos, como models, pra interfacear com os diversos serviços de IA como o chat GPT. A grosso modo, esses models seriam como um hibernate, uma ORM pra IAs. Mas tem outras abstrações. Tem agentes, tem correntes, tem índices. Mas uma das partes interessantes é que tem prompts. Diferente desses cursinhos idiotas por aí que ficam mais no esquema Olha só, eu testei uns prompts aqui e vou compartilhar com vocês. Ou então povo que fica copiando teorias da conspiração de prompt que surgem no Reddit. De fato, existem pesquisas acadêmicas sendo feitas no estudo das melhores formas de se fazer perguntas pra passar pra um transformador generativo como o GPT. Lembra o que eu falei? Os transformers só continuam adicionando palavras na frente do prompt que você escreveu. Quanto melhor escrito for o prompt, maiores as probabilidades de conseguir as respostas que procura. Ele vai conseguir prever melhor as palavras seguintes. O melhor prompt não é escrever um textão aleatório, mas sim conciso e estruturado. Na documentação eles fazem links pra diversos papers acadêmicos com o estudo pra cada tópico. Eu não levaria 100% a sério. Nem todos os papers têm consenso. Muita coisa é só teoremas e hipóteses. Mas é melhor do que chutes de youtuber. Com o Langchain eu poderia fazer um model que se integra com o meu Vicuna rodando localmente. Daí poderia não depender de serviços de terceiros e também manter a privacidade dos meus dados, já que eu não preciso compartilhar nada com ninguém. Se alguém tiver interesse de estudar, pesquisar ou até empreender com essa nova geração de transformadores, essas são ferramentas que podem ser muito úteis. Esquece curso idiota que é Duh, olha só como eu sou inteligente, sei integrar com a API da OpenAI. Sério, isso é o básico do básico do básico. Você aprende em 10 minutos num blog post. Não tem absolutamente nada de mais. Aliás, um curso de IA que se preza tem por obrigação ter tudo o que eu falei nesse vídeo, só que explicado com 10 vezes mais detalhes no mínimo. Caso contrário, não vale seu dinheiro e muito menos seu tempo. Não perca tempo com esses cursos caça-níquel. São todos perda de tempo. Veja os links que eu deixei na descrição do vídeo e estude um a um que no final você vai aprender muito melhor. E de graça. O hype em torno de IA tá fora de proporção. Um prato cheio pros oportunistas de plantão. Pra finalizar, acho que vale a pena voltar na questão que incomoda todo programador iniciante. Mas será que com essa evolução rápida de transformers não é questão de tempo até ele substituir todos os programadores? Eu já tinha feito um vídeo só pra responder isso e recomendo que assistam, mas considerando o que eu expliquei hoje, vou explicar porque tecnicamente isso não vai acontecer. Primeiro, um Transformer é incapaz de gerar textos sobre assuntos que nunca viu na vida. Isso é importante de entender. Vamos recordar o que é um modelo. É um banco de dados de tokens retirados dos textos de treinamento e mais importante, as probabilidades de um token pra outro token dentro de uma rede. O modelo não contém o texto original, só pesos. Vou repetir porque isso é importante. Nenhum texto aparece na sua forma original dentro do modelo. Pra ter na cabeça pense assim. Já viram que vários livros no final tem algumas páginas com índice remissivo? São todas as palavras importantes que aparecem no texto do livro e as páginas onde aparecem. É que nem um índice rudimentar de banco de dados. Quando fazemos um prompt e pedimos, sei lá, pra ele citar um trecho da peça King Lear de Shakespeare, vai gerar o trecho se não igual, muito próximo ao trecho do texto. E isso confunde, pois parece que então ele tem o texto inteiro guardado em algum lugar. Mas não. Por exemplo, eu pedi pra ele citar um diálogo entre Albany e Cordillia. E corretamente diz que não existe o diálogo entre os dois na peça e sugere outra cena, ato 4, cena 2, onde Albany fala com sua esposa Goneryl. E segue o trecho exatamente como tá na peça. Novamente, é muito difícil tratar o caminho exato dentro do modelo, indo de probabilidade em probabilidade pra reverter o processo e descobrir como o GPT conseguiu escrever o texto. Mas podemos dar um chute educado. A sequência de palavras do diálogo tem altíssima probabilidade. Primeiro no meu prompt tem todas as palavras-chave importantes pra contexto como Shakespeare, Albany e tudo mais. Quando ele começa digitando o trecho, you are, no contexto do diálogo, a chance maior é da próxima palavra ser notch, depois da palavra ser worth, depois ser the e depois dust. São as probabilidades mais altas dessa sequência. Ele não tem o texto original, mas tem as probabilidades que permitem remontar uma boa parte do texto original. Até certo ponto, a grosso modo, o modelo acaba servindo como uma versão comprimida dos textos originais, mas com perdas. Quebramos todos os textos em tokens e gravamos os relacionamentos de forma que é possível recuperar alguns desses textos. Mas não podemos garantir que é possível recuperar tudo, porque a mistura de alguns textos similares vai desajustar as probabilidades. No final teríamos no máximo a versão mesclada de dois ou mais textos. Quanto mais repetidas vezes um determinado texto aparece em diversas fontes no material de treinamento, maiores as chances de conseguir reconstruir depois. Shakespeare, tendo obras que foram analisadas, discutidas e citadas inúmeras vezes em várias fontes, tem maiores chances de ser reconstruído. Sobre mim, Fábio Akita, já vai ser mais difícil, porque no contexto geral da web eu sou extremamente pouco citado. Mesmo assim o GPT-4 até que consegue fazer uma descrição super genérica sobre mim. Fala coisas certas, por exemplo, que eu sou brasileiro, que fundei a CodeMiner, que palestrei em eventos e ajudei a divulgar o Ruby on Rails. Mas aí fala que eu também ajudei a divulgar metodologias ágeis, que não é mentira, mas nunca foi um ponto importante. Ele fala que eu fui Keynote Speaker, ou seja, que dei palestras de abertura em muitas conferências, o que não é verdade. Ele já começou a misturar informações de outras pessoas similares. Mas isso não é acidente, é porque comparado a um Shakespeare não tem quase nada sobre mim na web, e as probabilidades são muito parecidas com outros palestrantes de tecnologia da mesma área. Por isso também é difícil pra ele citar fontes exatas, porque cada palavra do texto que gera pode ter vindo de um lugar diferente, não tem como saber exatamente. Pra ter links pra fontes como o Bing faz, eu especulo que precisei de um processo em duas ou mais etapas. Primeiro, gerar o texto da resposta como um GPT já faz. Depois pegar esse texto e fazer alguma pesquisa tradicional no Bing antigo. Daí cruzar as duas informações e ver se os textos minimamente batem e finalmente apresentar juntas como se a resposta já tivesse saído pronto com os links pras fontes. Não é o Transformer que dá os links, necessariamente. É um segundo processo separado. Porque o modelo do Transformer em si não tem como garantir exatamente qual parte veio de onde. Quanto mais um certo assunto tiver textos pra treinar, mais precisas vão ser as respostas. Por isso é mais fácil conseguir informações sobre celebridades do que sobre alguém como eu ou vocês. No nosso caso, nossas informações vão ser misturadas com de outras pessoas similares porque as probabilidades não vão ajudar. Lembra a temperatura de criatividade? Mas não tem jeito. Toda resposta sobre coisas que não tinha no material original de treinamento ou tinha muito pouco vai ser uma alucinação probabilística. Só isso. E vai estar errado. Isso acontece com tudo que for novidade. No dia 1 todo o Transformer vai ser praticamente inútil. Outra coisa. Mesmo sendo capaz de ler código, explicar código dos outros e gerar códigos, ainda não vai ter a capacidade de gerar um projeto inteiro. Lembra os códigos vazados do Twitch ou o código de ranking do Twitter que eu analisei? Estamos falando de milhares de arquivos, centenas de milhares de linhas de código. Sabe qual é um dos calcanhares de Aquiles de toda a IA, backtracking e memória? As respostas costumam ser boas hoje porque ele leva em consideração o texto anterior. Em partes. Existe um limite de quanto consegue voltar atrás no texto anterior. No caso do GPT-3 e 4 atual, ele se limita a enxergar uma janela de contexto de no máximo 2048 tokens. E as respostas também tem um limite. Quanto mais longo for esse limite, mais caro fica pra processar o resultado seguinte do gerador de palavras. Maiores as chances dele esquecer o contexto. E muito maiores as chances de rapidamente perder a coerência de respostas longas. Entendam, o GPT atual não tem capacidade de ler mais que 2048 tokens. Então jogar o código inteiro da kernel do Linux pra ele analisar, por exemplo, é impossível. E pedir pra ele escrever um código desse mesmo tamanho é mais impossível ainda. Lógico, nada é impossível, mas é altamente improvável. No meu Vicuna rodando o local tem essa tela de configuração pra justamente configurar o tamanho da janela de contexto e de resposta. E não é um número muito grande não. Num teste não científico eu tentei abrir duas janelas do projeto Text Generation, que é a interface web que eu tô usando em cima do Vicuna. Não sei se é uma precaução da interface web ou uma limitação do próprio Vicuna por baixo, mas só tem capacidade de responder uma pergunta de cada vez. Se eu abrir outro navegador e tentar fazer outra pergunta enquanto na outra janela e ainda não terminou de responder a primeira, vai ignorar e não deixar rodar em paralelo. E mesmo se conseguisse, só com um único processo de resposta, tá consumindo no mínimo 50% da minha GPU consumindo mais de 18GB de VRAM. E enquanto responde esse consumo continua aumentando, ou seja, numa máquina como a minha ele só consegue responder uma pergunta de cada vez. Você tá pensando em fazer um serviço online, SAS pra vários usuários acessarem? Vai sair super caro, porque só vai possibilitar uma resposta a cada 2 a 5 segundos. Um servidor web pequeno tem capacidade pra responder, sei lá, 1000 requisições por segundo ou mais. Mas um Vicuna tá mais pra 0.2 requisições por segundo. É ridiculamente pesado e não escala. Eu não lembro que artigo que a OpenAI falava que cada dia de operação do chat GPT custa sei lá, 1 milhão de dólares. Sim, 1 milhão por dia. Mais ou menos faz sentido, dado o custo de GPU necessário pra responder todo mundo. Mas claro, novas técnicas de otimização tem aparecido, como o exemplo de quantização que eu expliquei antes. Mas estamos falando que precisa melhorar ainda umas 10 mil vezes pra ficar economicamente viável pra casos como escrever um livro de 500 páginas sem perder coerência ou escrever um projeto de verdade com milhares de arquivos de código. No estágio atual da tecnologia é impossível. Essas são só algumas das razões de por que um chat GPT não vai substituir programadores e outras profissões. A partir de um certo ponto fica tão caro que não compensa, especialmente porque como eu já expliquei, essa inteligência não é inteligente. A gente só cospia probabilidades, não tem criatividade, não sabe tomar decisões baseadas em coisas como custo-benefício ou bom senso. Não vai ter coerência em respostas longas, não tem noção se o código sendo cuspido tem segurança ou escalabilidade. Ela tem memória menor que um peixinho dourado, 2.048 tokens. Mesmo se aumentar 10 vezes esse limite ainda é super pouco. O script deste episódio tem 15 mil palavras. O chat GPT é incapaz de escrever meus scripts. Mas tudo bem. Como uma ferramenta de auxílio é super boa, trechos pequenos e repetitivos, código trivial que facilmente encontramos no stack overflow repetidas vezes, o estado atual de transformers é mais que suficiente pra acelerar um bom profissional. Hoje é impossível e vai continuar sendo impossível alguém que não é programador conseguir fazer um projeto complexo inteiro. Qualquer um que diga o contrário não tem a menor noção do que está falando. Considere como os algoritmos funcionam e considere os pontos de máximo e mínimo. Não basta dizer Ah, eu tenho esperança, é só ter fé. Precisamos de números. E os números dizem que é extremamente improvável. Eu uso o GitHub Copilot faz meses. Em vários vídeos, toda vez que aparecem trechos de código, eu usei a ajuda do chat GPT ou Copilot. Ambos tem plugins pra vários editores como NeoVim ou VSCode e funcionam super bem. Tem a capacidade de ler o código do arquivo que se tá editando. Não consegue usar o projeto inteiro de contexto pelos limites que eu acabei de explicar, mas só de usar o trecho próximo do que tá digitando já ajuda muito. Ou seja, parte do prompt é o código do arquivo aberto naquele momento. Pra mim vale cada centavo. E todo desenvolvedor que quiser ter vantagens de produtividade deveria considerar usar esses plugins. De novo, não vai conseguir fazer tudo pra você e por causa da aleatoriedade e alto risco de alucinações, você nunca deve aceitar o código que ele sugere sem ler com muita calma antes. Mas pra tarefas bem braçais, tediosas, onde faremos muito copy-paste como fazer testes unitários simples, ajuda. Testem. Com tudo isso que eu expliquei, minha conclusão pessoal é que não estamos nada perto do que o povo chama de AGI ou Inteligência Artificial Geral. A IA que vai superar todas as IAs. A IA que vai pegar o código inteiro do GPT-4 e sozinho gerar um GPT-5 melhorado. O coitado male male tem capacidade de lembrar um único arquivo de código que dirá milhares. E mesmo se conseguisse, ele não estaria analisando. Ele não analisa nada. Ele não sabe analisar. Ele não sabe as regras de cálculo. As regras de programação. Só sabe juntar pedaços que viu repetidas vezes de muitas fontes. 100% via probabilidades. E quando não sabe vai pegar as probabilidades mais próximas e cuspir o que der. E vai ser um texto sem sentido nenhum. Eu vou repetir. Transformers não tem inteligência nenhuma de análise e cognição. Ele não sabe porque 2 mais 2 é 4. Só leu muitas vezes que é. E as probabilidades fazem ele repetir isso. GPT quer dizer Transformer Generativo Pré-treinado. Hoje você entendeu o que esses termos significam. E se ficou até aqui, eu espero que tenham entendido. Por mais impressionante que pareça os textos que gera, é só um gerador de textos. Um autocorretor de teclado de celular glorificado. Um gerador de textos nunca vai ser inteligente. Só vai parecer inteligente. Assim como o formato de animal que você viu numa nuvem foi totalmente um acidente. Ninguém teve a intenção de desenhar um animal na nuvem. Sequer nem um animal. É só um formato aleatório. Foi você que escolheu entender assim. E foi por isso que no outro episódio de ChatGPT eu afirmei e repito. O seu grau de empolgação com a IA é inversamente proporcional ao seu entendimento de IA. Quanto menos entender, mais empolgado vai ficar. Vai acreditar que as ações da Nvidia estarem a mais de 400 dólares é normal. Porque acredita que negócios de IA vão aumentar absurdamente e em breve a ação vai atingir 1000 dólares. Vai investir todo o seu dinheiro nisso. Já eu acho que a ação da Nvidia está hipervalorizada por puro hype. O preço justo da ação deveria ser abaixo de 300 dólares. E vai voltar pra isso uma hora. Eu não sei quando vai estourar, mas eu já estou precavido. Eu não tenho dúvida que transformers são úteis. Várias ferramentas já estão usando. Em particular um Creative Cloud da Adobe está tirando bom proveito. A Microsoft vai embutir em Office, Windows e tudo mais. Mas no final é isso que a gente vai ter. Um Alexa melhorado, um template mais inteligente de Excel. Mas não acha que vai substituir o seu diretor financeiro e que ele vai conseguir tomar decisões inteligentes. Só parece. Por sorte e por acidente. Quem apostar em mais do que isso nas tecnologias atuais vai perder. Pessoalmente eu acho que as tecnologias de Transformers e outras que nem mencionei hoje, que compõem o que é o GPT, assim como tudo, segue uma curva em S. Passamos pela parte do S que é o crescimento que parece exponencial. Mas a curva tem um teto em cima. Quando pulamos de bilhões de parâmetros pra trilhões de parâmetros, vão começar a ter retornos diminuídos. Diminishing Returns. Se amanhã sair um GPT-5 com o dobro de parâmetros, sei lá, 300 trilhões, não vai ficar duas vezes melhor. Talvez melhore sei lá, 20%. Cada melhoria custa mais caro do que o retorno. Isso é Diminishing Returns. Não existe crescimento infinito. E quanto mais rápido você força, só chega mais rápido no teto. Precisa acontecer novas descobertas, novas invenções que ainda não conhecemos, obviamente, pra ter uma nova etapa de evolução significativa. Enquanto isso não acontecer, e só insistir no que temos até hoje, não vai ser muito melhor que isso não. Já estamos perto do limite e a AGI não está no horizonte ainda. É a mesma coisa ficar especulando. Estamos perto de conseguir construir uma USS emissão. E uma USS Enterprise. É irrelevante. Beira ficção científica. E eu acho que já me estendi demais. Como falei no começo do vídeo, eu não sou nem de longe um especialista no assunto, sequer tenho experiência prática de trabalhar em projetos relacionados. O meu conhecimento até agora é puramente teórico. Eu só pesquisei o que existe publicado online. Se eu entendi até aqui em poucos dias, quem dedicar algumas semanas vai acabar sabendo muito mais do que eu muito rapidamente. Não é tão difícil assim. Espero que tenha dado pra quebrar alguns mitos que tinham na cabeça e finalmente tenham conseguido separar o joio do trigo. Se ficaram com dúvida ou quiserem complementar, fique à vontade nos comentários abaixo. Se curtir o vídeo deixe um joinha, assine o canal e não deixe de compartilhar o vídeo com seus amigos. A gente se vê, até mais! Até mais! A a a a... A a a... A a a... A a... A a... Existem... Conseguem ver as similar owned entre o... Conseguem ver as similar owned entre o alto... Conseguem ver as similar owned entre o alto... Caralho, alto. Alto que alto. Dizem que... O de 13 bilhões de parâmetros... Parâmetros, parâmetros... probabilidade final da compu... custou usar de emp... rnn bptt bptt seria absolutamente... absurdamente difícil de entender isso e eu também vou explicar Em sequência linear o cacete tem metáfora Parágrafo, parágrafo, caralho. Instruções SIMD. CPUs, diferentes de GPUs. Cacete. Usamos vocabulário neurológico. Neurológico. Moser. Temos que... 65 bilhões até... Nada mais, nada mesmo. Nada menos. Menos. Puta que pariu. Conceito de GANs. Generative... Generative... O que me leva a outro... O que me leva... Mas é melhor do que chutes do youtuber. Chutes de youtuber. A cidade dos meus dados. Todas as palavras-chave importantes. Para... Ou código de ranking. Ranking. Ranking. Ranking. Puta que pariu. Que vídeo longo da porra. O que? O que? Que vídeoya pretende. Tela.ifs... O que? Pocait Judge. É barato. Terra. Mólico de chat. Incочь. Sola. Poucait.